{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReinforcementLearning\n",
    "using Flux\n",
    "using Statistics\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CartPoleEnv{Float32}(gravity=9.8,masscart=1.0,masspole=0.1,totalmass=1.1,halflength=0.5,polemasslength=0.05,forcemag=10.0,tau=0.02,thetathreshold=0.20943952,xthreshold=2.4,max_steps=200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = CartPoleEnv(;T=Float32, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DQN\n",
    "    TRAIN::Bool\n",
    "    CHANGE::Bool\n",
    "    ϵ::Float64\n",
    "    ϵ_DECAY::Float64\n",
    "    ϵ_MIN::Float64\n",
    "    BATCH_SIZE::Int64\n",
    "    MEMORY\n",
    "    MEM_SIZE::Int64\n",
    "    STATE_SIZE::Int64\n",
    "    ACTION_SIZE::Int64\n",
    "    γ::Float64\n",
    "    C_UPDATE::Int64\n",
    "    model1\n",
    "    model2\n",
    "    \n",
    "    function DQN(ϵ_DECAY::Float64, ϵ_MIN::Float64,BATCH_SIZE::Int64,MEM_SIZE::Int64,STATE_SIZE::Int64,\n",
    "            \n",
    "                 ACTION_SIZE::Int64,γ::Float64,C_UPDATE::Int64, model1)\n",
    "        \n",
    "        new(true, true, 1.0f0, ϵ_DECAY, ϵ_MIN, BATCH_SIZE, [], MEM_SIZE, STATE_SIZE, ACTION_SIZE,γ,C_UPDATE\n",
    "            \n",
    "            model1, deepcopy(model2))\n",
    "    end\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: incomplete: premature end of input",
     "output_type": "error",
     "traceback": [
      "syntax: incomplete: premature end of input",
      ""
     ]
    }
   ],
   "source": [
    "#Fonctions élémentaires\n",
    "\n",
    "function action(dqn::DQN,state::Array{Float32,1})\n",
    "\n",
    "  if rand() <= dqn.ϵ && dqn.TRAIN\n",
    "        \n",
    "    return rand(1:dqn.ACTION_SIZE)\n",
    "\n",
    "  end\n",
    "\n",
    "\n",
    "\n",
    "  act_values = dqn.model1(state)\n",
    "\n",
    "  return Flux.argmax(act_values)\n",
    "    \n",
    "end\n",
    "\n",
    "function update_ϵ!(dqn::DQN)\n",
    "    \n",
    "    x = dqn.ϵ*dqn.ϵ_DECAY\n",
    "    \n",
    "    if x < dqn.ϵ_MIN && dqn.CHANGE\n",
    "        \n",
    "        dqn.ϵ=dqn.ϵ_MIN\n",
    "        \n",
    "        dqn.CHANGE=false\n",
    "        \n",
    "        \n",
    "    elseif dqn.CHANGE\n",
    "        \n",
    "        dqn.ϵ = x\n",
    "        \n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "        \n",
    "function act(action::Int64, env)\n",
    "    \n",
    "    env(action)\n",
    "    \n",
    "    \n",
    "    obs=observe(env)\n",
    "    \n",
    "    get_state(obs), get_reward(obs), get_terminal(obs)\n",
    "    \n",
    "end\n",
    "\n",
    "    \n",
    "    \n",
    "function remember!(dqn::DQN,state::Array{Float32,1}, action::Int64, reward::Int64, next_state::Array{Float32,1}, done::Bool)\n",
    "\n",
    "  if length(dqn.memory) == dqn.MEM_SIZE\n",
    "\n",
    "    deleteat!(dqn.memory, 1)\n",
    "\n",
    "  end\n",
    "\n",
    "  push!(dqn.memory, (state, action, reward, next_state, done))\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function replay!(dqn::DQN)\n",
    "\n",
    "  batch_size = min(dqn.BATCH_SIZE, length(dqn.memory))\n",
    "\n",
    "  minibatch = sample(dqn.memory, batch_size, replace = false)\n",
    "\n",
    "  \n",
    "\n",
    "  x = Matrix{Float32}(dqn.STATE_SIZE, batch_size)\n",
    "\n",
    "  y = Matrix{Float32}(dqn.ACTION_SIZE, batch_size)\n",
    "\n",
    "  for (iter, (state, action, reward, next_state, done)) in enumerate(minibatch)\n",
    "\n",
    "    target = reward\n",
    "\n",
    "    if !done\n",
    "\n",
    "      target += dqn.γ * maximum(dqn.model2(next_state))\n",
    "\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "    target_f = dqn.model1(state)\n",
    "\n",
    "    target_f[action] = target\n",
    "\n",
    "    \n",
    "\n",
    "    x[:, iter] .= state\n",
    "\n",
    "    y[:, iter] .= target_f\n",
    "\n",
    "  end\n",
    "\n",
    "\n",
    "  Flux.train!(loss, [(x, y)], opt)\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function copy(iter::Int64, dqn::DQN)\n",
    "    \n",
    "    if iter%dqn.C_UPDATE==0\n",
    "        \n",
    "        dqn.model2 = deepcopy(dqn.model1)\n",
    "        \n",
    "    end\n",
    "    \n",
    "end\n",
    "    \n",
    "    \n",
    "#Run 1 épisode\n",
    "\n",
    "function episode!(dqn::DQN, env)\n",
    "    \n",
    "    obs_ini=observe(env)\n",
    "    \n",
    "    current_state=get_state(obs_ini)\n",
    "    \n",
    "    \n",
    "    total_reward=0\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while true\n",
    "        \n",
    "        current_action=action(dqn,current_state)\n",
    "        \n",
    "        current_next_state,current_reward,current_done=act(current_action,env)\n",
    "        \n",
    "        \n",
    "        total_reward+=current_reward\n",
    "        \n",
    "        \n",
    "        remember!(dqn, current_state, current_action, current_reward, current_next_state, current_done)\n",
    "        \n",
    "        current_state=current_next_state\n",
    "        \n",
    "        replay!(dqn)\n",
    "        \n",
    "        update_ϵ!(dqn)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        copy(i,dqn)\n",
    "        \n",
    "        if done\n",
    "            \n",
    "            break\n",
    "            \n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    total_reward\n",
    "    \n",
    "end\n",
    "        \n",
    "\n",
    "#Run DQN algorithm\n",
    "\n",
    "function main_DQN(dqn::DQN, env)\n",
    "    \n",
    "    e = 1\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    while true\n",
    "\n",
    "      reset!(env)\n",
    "\n",
    "      total_reward = episode!(dqn,env)\n",
    "\n",
    "      push!(scores, total_reward)\n",
    "\n",
    "      print(\"Episode: $e | Score: $total_reward \")\n",
    "\n",
    "      if e > 100\n",
    "\n",
    "        last_100_mean = mean(scores[end-99:end])\n",
    "\n",
    "        print(\"Last 100 episodes mean score: $last_100_mean\")\n",
    "\n",
    "        if last_100_mean > 195\n",
    "\n",
    "          println(\"\\nProblem solved!\")\n",
    "\n",
    "          break\n",
    "\n",
    "        end\n",
    "        \n",
    "        if e > 200\n",
    "                \n",
    "          println(\"\\nProblem unsolved!\")\n",
    "                \n",
    "          break\n",
    "        \n",
    "        end\n",
    "\n",
    "      end\n",
    "\n",
    "      println()\n",
    "\n",
    "      e += 1\n",
    "\n",
    "    end\n",
    "    \n",
    "    e, scores\n",
    "    \n",
    "end\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
